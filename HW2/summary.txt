########################
# Additional Files
########################
# readme.md

########################
# Filled Code
########################
# ..\codes\mlp\model.py:1
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        # momentum for `Exponential moving average`, see "https://en.wikipedia.org/wiki/Moving_average"
        self.momentum = 0.9
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        if self.training:
            mean = torch.mean(input)
            var = torch.var(input, unbiased=False)
            input = (input - mean) / torch.sqrt(var + 1e-5)
            input = self.weight * input + self.bias
            self.running_mean = self.momentum * self.running_mean + (1.0 - self.momentum) * mean
            self.running_var = self.momentum * self.running_var + (1.0 - self.momentum) * var
        else:
            mean = self.running_mean
            var = self.running_var
            input = (input - mean) / torch.sqrt(var + 1e-5)
            input = self.weight * input + self.bias

# ..\codes\mlp\model.py:2
        self.remain_rate_reverse = 1. / (1. - self.p)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if self.training:
            prob = torch.empty(*(input.size()), device=self.device).fill_(self.p)
            mask = torch.bernoulli(prob)
            input[mask == 1.] = 0.
            input *= self.remain_rate_reverse

# ..\codes\mlp\model.py:3
        self.hidden_size = 784
        self.mlp = nn.Sequential(
            nn.Linear(32*32*3, self.hidden_size),
            BatchNorm1d(self.hidden_size),
            nn.ReLU(),
            Dropout(drop_rate),
            nn.Linear(self.hidden_size, 10),
        )

# ..\codes\mlp\model.py:4
        y = y.long()
        logits = self.mlp(x)

# ..\codes\cnn\model.py:1
        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))
        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))
        # momentum for `Exponential moving average`, see "https://en.wikipedia.org/wiki/Moving_average"
        self.momentum = 0.9
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        _, C, H, W = input.shape
        # remain dimension: C
        input_flat = input.permute(1, 0, 2, 3).reshape(C, -1)

        if self.training:
            mean = torch.mean(input_flat, dim=1)
            var = torch.var(input_flat, unbiased=False, dim=1)
            # input = (input - mean.view(1, C, 1, 1)) / torch.sqrt(var.view(1, C, 1, 1) + 1e-5)
            input = (input - mean.view(1, C, 1, 1)) / torch.sqrt(var.view(1, C, 1, 1) + 1e-5)
            input = (self.weight * input + self.bias)
            self.running_mean = self.momentum * self.running_mean + (1.0 - self.momentum) * mean
            self.running_var = self.momentum * self.running_var + (1.0 - self.momentum) * var
        else:
            mean = self.running_mean
            var = self.running_var
            input = (input - mean.view(1, C, 1, 1)) / torch.sqrt(var.view(1, C, 1, 1) + 1e-5)
            input = (self.weight * input + self.bias)

# ..\codes\cnn\model.py:2
        self.remain_rate_reverse = 1. / (1. - self.p)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if self.training:
            prob = torch.empty(*(input.size()), device=self.device).fill_(self.p)
            mask = torch.bernoulli(prob)
            input[mask == 1.] = 0.
            input *= self.remain_rate_reverse

# ..\codes\cnn\model.py:3
        self.out_channels = [300, 300]
        self.kernel_sizes = [5, 3]
        self.cnn_out_size = (((32 - self.kernel_sizes[0] + 1) // 2) - self.kernel_sizes[1] + 1) // 2
        # out = (in - fliter + 2*padding) // stride + 1
        # for default torch definition:
        #	Conv2d:     out = in - fliter + 1
        #   MaxPool2d:  out = in // fliter
        # in: 3 x 32 x 32
        self.cnn = nn.Sequential(
            nn.Conv2d(3, self.out_channels[0], self.kernel_sizes[0]),  # out: 300 x 28 x 28
            BatchNorm2d(self.out_channels[0]),
            nn.ReLU(),
            Dropout(drop_rate),
            nn.MaxPool2d(2),  # out: 300 x 14 x 14
            nn.Conv2d(self.out_channels[0], self.out_channels[1], self.kernel_sizes[1]),  # out: 300 x 12 x 12
            BatchNorm2d(self.out_channels[1]),
            nn.ReLU(),
            Dropout(drop_rate),
            nn.MaxPool2d(2),  # out: 300 x 6 x 6 = 10800
        )
        self.final_layer = nn.Linear(self.out_channels[1] * self.cnn_out_size * self.cnn_out_size, 10)

# ..\codes\cnn\model.py:4
        y = y.long()
        feature = self.cnn(x)
        logits = self.final_layer(feature.view(feature.shape[0], -1))


########################
# References
########################

########################
# Other Modifications
########################
# _codes\mlp\main.py -> ..\codes\mlp\main.py
# 108 -         mlp_model = Model(drop_rate=drop_rate)
# 108 +         mlp_model = Model(drop_rate=args.drop_rate)
# 108 ?                                     +++++

